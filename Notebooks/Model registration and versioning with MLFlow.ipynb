{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df2981f",
   "metadata": {},
   "source": [
    "# Model registration and versioning with MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8937b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, roc_auc_score, precision_score, recall_score,roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from urllib.parse import urlparse\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f6568e6-6d9e-4380-82d1-be2a8b406c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_proba(actual, proba):\n",
    "    roc_auc = roc_auc_score(actual, proba)\n",
    "    average_precision= average_precision_score(actual, proba)\n",
    "    \n",
    "    return roc_auc,average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "276baf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_pred(actual, pred):\n",
    "   \n",
    "    accuracy= accuracy_score(actual, pred)\n",
    "    precision= precision_score(actual, pred)\n",
    "    recall= recall_score(actual, pred)\n",
    "    f1= f1_score(actual, pred)\n",
    "        \n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "373929a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc: 0.8190164137120954\n",
      "average_precision: 0.7797528345433136\n",
      "accuracy: 0.7638888888888888\n",
      "precision: 0.7602040816326531\n",
      "recall: 0.7967914438502673\n",
      "f1 score: 0.7780678851174935\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Read the wine-quality csv file from the URL\n",
    "    csv_url = (\n",
    "        \"C:/Users/willi/Python/Spotify_Project/Data/preprocess_data.csv\"\n",
    "    )\n",
    "    try:\n",
    "        data = pd.read_csv(csv_url, sep=\",\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.exception(\n",
    "            \"Unable to download training & test CSV, check your internet connection. Error: %s\", e\n",
    "        )\n",
    "\n",
    "    # Split the data into training and test sets. (0.8, 0.2) split.\n",
    "    train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "    # The predicted column is \"mood\" which is a binary [0, 1]\n",
    "    train_x = train.drop(\"mood\", axis=1)\n",
    "    test_x = test.drop(\"mood\", axis=1)\n",
    "    train_y = train[[\"mood\"]]\n",
    "    test_y = test[[\"mood\"]]\n",
    "\n",
    "   \n",
    "    with mlflow.start_run():\n",
    "        \n",
    "        clf = lgb.LGBMClassifier()\n",
    "        clf.fit(train_x, train_y)\n",
    "\n",
    "        y_pred = clf.predict(test_x)\n",
    "        y_proba = clf.predict_proba(test_x)[:, 1]\n",
    "        \n",
    "        (roc_auc, average_precision) = eval_proba(test_y, y_proba)\n",
    "        (accuracy, precision, recall, f1) = eval_pred(test_y, y_pred)\n",
    "\n",
    "        #print( Lgbm model (alpha=%f, l1_ratio=%f):\" % (alpha, l1_ratio))\n",
    "        print(\"roc_auc: %s\" % roc_auc)\n",
    "        print(\"average_precision: %s\" % average_precision)\n",
    "        print(\"accuracy: %s\" % accuracy)\n",
    "        print(\"precision: %s\" % precision)\n",
    "        print(\"recall: %s\" % recall)\n",
    "        print(\"f1 score: %s\" % f1)\n",
    "\n",
    "        #mlflow.log_param(\"alpha\", alpha)\n",
    "        #mlflow.log_param(\"l1_ratio\", l1_ratio)\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "        mlflow.log_metric(\"average_precision\", average_precision)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"f1 score\", f1)\n",
    "\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "        # Model registry does not work with file store\n",
    "        if tracking_url_type_store != \"file\":\n",
    "\n",
    "            # Register the model\n",
    "            # There are other ways to use the Model Registry, which depends on the use case,\n",
    "            # please refer to the doc for more information:\n",
    "            # https://mlflow.org/docs/latest/model-registry.html#api-workflow\n",
    "            mlflow.sklearn.log_model(clf, \"model\", registered_model_name=\"LgbmModel\")\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(clf, \"model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
