{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import optuna\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,ExtraTreesClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV, train_test_split, KFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Preprocess data\n",
    "X_train = pd.read_csv(\"C:/Users/willi/Python/Spotify_Project/Data/X_train.csv\")\n",
    "y_train = pd.read_csv(\"C:/Users/willi/Python/Spotify_Project/Data/X_test.csv\")\n",
    "X_test = pd.read_csv(\"C:/Users/willi/Python/Spotify_Project/Data/y_train.csv\")\n",
    "y_test = pd.read_csv(\"C:/Users/willi/Python/Spotify_Project/Data/y_test.csv\")\n",
    "train = pd.read_csv(\"C:/Users/willi/Python/Spotify_Project/Data/preprocess_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(\"mood\", axis=1)\n",
    "y = train[\"mood\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"https://dagshub.com/inouyewilliam/Master-Thesis.mlflow\"\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"inouyewilliam\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] =\"b185d44c9fe85ded477875ff2ba1b4d229006006\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, X_test, y_test):\n",
    "    # Evaluate the model using cross-validation\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5)\n",
    "    cv_score = np.mean(cv_scores)\n",
    "    \n",
    "    # Get the model predictions and probabilities\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate the evaluation metrics\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "    avg_precision= average_precision_score(y_test, y_proba)\n",
    "    accuracy= accuracy_score(y_test, y_pred)\n",
    "    precision= precision_score(y_test, y_pred)\n",
    "    recall= recall_score(y_test, y_pred)\n",
    "    f1= f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Return a dictionary of evaluation metrics\n",
    "    return {\n",
    "        \"cv_score\": cv_score,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"average_precision\": avg_precision,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-08 22:11:40,031]\u001b[0m A new study created in memory with name: no-name-e6fd96be-fc38-4c8e-8ce2-8992ebb29d3e\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:11:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"num_leaves\" } are not used.\n",
      "\n",
      "[22:11:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"num_leaves\" } are not used.\n",
      "\n",
      "[22:11:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"num_leaves\" } are not used.\n",
      "\n",
      "[22:11:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"num_leaves\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-05-08 22:11:42,253]\u001b[0m Trial 0 failed with parameters: {'k': 11, 'et_n_estimators': 121, 'et_max_depth': 10, 'lgbm_max_depth': 5, 'lgbm_n_estimators': 90, 'lgbm_num_leaves': 19, 'xgb_max_depth': 7, 'xgb_n_estimators': 79, 'xgb_num_leaves': 45} because of the following error: KeyError('et').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\willi\\anaconda3\\envs\\mlops\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\willi\\AppData\\Local\\Temp\\ipykernel_7308\\3625654654.py\", line 49, in objective\n",
      "    best_pipeline = best_pipelines[best_algo][\"pipeline\"]\n",
      "                    ~~~~~~~~~~~~~~^^^^^^^^^^^\n",
      "KeyError: 'et'\n",
      "\u001b[33m[W 2023-05-08 22:11:42,256]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:11:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"num_leaves\" } are not used.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'et'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 78\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run():\n\u001b[0;32m     76\u001b[0m     \u001b[39m# Optimize the hyperparameters using Optuna\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 78\u001b[0m     study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[0;32m     80\u001b[0m     \u001b[39m# Get the best hyperparameters and score\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n",
      "File \u001b[1;32mc:\\Users\\willi\\anaconda3\\envs\\mlops\\Lib\\site-packages\\optuna\\study\\study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \n\u001b[0;32m    334\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     _optimize(\n\u001b[0;32m    426\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    427\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    428\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    429\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    430\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    431\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    432\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    433\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    434\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    435\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\willi\\anaconda3\\envs\\mlops\\Lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\willi\\anaconda3\\envs\\mlops\\Lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\willi\\anaconda3\\envs\\mlops\\Lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\willi\\anaconda3\\envs\\mlops\\Lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[33], line 49\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39m# Choose the best algorithm based on the cross-validation scores\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     best_algo \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(scores, key\u001b[39m=\u001b[39mscores\u001b[39m.\u001b[39mget)\n\u001b[1;32m---> 49\u001b[0m     best_pipeline \u001b[39m=\u001b[39m best_pipelines[best_algo][\u001b[39m\"\u001b[39m\u001b[39mpipeline\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     50\u001b[0m     best_params \u001b[39m=\u001b[39m {\n\u001b[0;32m     51\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mk\u001b[39m\u001b[39m\"\u001b[39m: k,\n\u001b[0;32m     52\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mbest_algo\u001b[39m}\u001b[39;00m\u001b[39m_best_params\u001b[39m\u001b[39m\"\u001b[39m: best_pipeline\u001b[39m.\u001b[39mnamed_steps[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget_params(),\n\u001b[0;32m     53\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mbest_algo\u001b[39m}\u001b[39;00m\u001b[39m_mean_cv_score\u001b[39m\u001b[39m\"\u001b[39m: scores[best_algo],\n\u001b[0;32m     54\u001b[0m                     }\n\u001b[0;32m     55\u001b[0m     \u001b[39m# Save the best pipeline for each algorithm in MLflow\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'et'"
     ]
    }
   ],
   "source": [
    "# Define the objective function to be optimized by Optuna\n",
    "def objective(trial):\n",
    "    k = trial.suggest_int(\"k\", 5, X.shape[1])\n",
    "\n",
    "    # Define the pipelines with different algorithms\n",
    "    pipelines = {\n",
    "        \"et\": Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"selector\", SelectKBest(f_classif, k=k)),\n",
    "            (\"model\", ExtraTreesClassifier(\n",
    "                n_estimators=trial.suggest_int(\"et_n_estimators\", 50, 200),\n",
    "                max_depth=trial.suggest_int(\"et_max_depth\", 5, 20),\n",
    "            )),\n",
    "        ]),\n",
    "        \"lgbm\": Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"selector\", SelectKBest(f_classif, k=k)),\n",
    "            (\"model\", LGBMClassifier(\n",
    "                #learning_rate=trial.suggest_loguniform(\"lgbm_learning_rate\", 1e-3, 1e-1),\n",
    "                max_depth=trial.suggest_int(\"lgbm_max_depth\", 3, 10),\n",
    "                n_estimators=trial.suggest_int(\"lgbm_n_estimators\", 50, 200),\n",
    "                num_leaves=trial.suggest_int(\"lgbm_num_leaves\", 2, 50),\n",
    "            )),\n",
    "        ]),\n",
    "        \"xgb\": Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"selector\", SelectKBest(f_classif, k=k)),\n",
    "            (\"model\", XGBClassifier(\n",
    "                #learning_rate=trial.suggest_loguniform(\"xgb_learning_rate\", 1e-3, 1e-1),\n",
    "                max_depth=trial.suggest_int(\"xgb_max_depth\", 3, 10),\n",
    "                n_estimators=trial.suggest_int(\"xgb_n_estimators\", 50, 200),\n",
    "                num_leaves=trial.suggest_int(\"xgb_num_leaves\", 2, 50),\n",
    "            )),\n",
    "        ]),\n",
    "    }\n",
    " # Train and evaluate each pipeline using cross-validation\n",
    "    scores = {}\n",
    "    best_pipelines = {}\n",
    "    for algo, pipeline in pipelines.items():\n",
    "            score = np.mean(cross_val_score(pipeline, X, y, cv=5))\n",
    "            scores[algo] = score\n",
    "        \n",
    "# Save the best pipeline for each algorithm and their corresponding scores\n",
    "    if algo not in best_pipelines or score > best_pipelines[algo][\"score\"]:\n",
    "        best_pipelines[algo] = {\"pipeline\": pipeline, \"score\": score}\n",
    "    \n",
    "# Choose the best algorithm based on the cross-validation scores\n",
    "    best_algo = max(scores, key=scores.get)\n",
    "    best_pipeline = best_pipelines[best_algo][\"pipeline\"]\n",
    "    best_params = {\n",
    "            \"k\": k,\n",
    "            f\"{best_algo}_best_params\": best_pipeline.named_steps[\"model\"].get_params(),\n",
    "            f\"{best_algo}_mean_cv_score\": scores[best_algo],\n",
    "                    }\n",
    "    # Save the best pipeline for each algorithm in MLflow\n",
    "    best_mean_cv_score = None\n",
    "    with mlflow.start_run(nested=True):\n",
    "        for algo, pipeline_info in best_pipelines.items():\n",
    "            pipeline = pipeline_info[\"pipeline\"]\n",
    "            pipeline_name = f\"{algo}_pipeline\"\n",
    "            mlflow.sklearn.log_model(pipeline, pipeline_name)\n",
    "\n",
    "            # Log the best params and score for this pipeline\n",
    "            if algo == best_algo:\n",
    "                mlflow.log_params(best_params)\n",
    "                mlflow.log_metric(\"mean_cv_score\", scores[best_algo])\n",
    "\n",
    "    # Return the mean cross-validation score of the best algorithm\n",
    "    return best_mean_cv_score\n",
    "\n",
    "# Set up MLflow tracking\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/inouyewilliam/Master-Thesis.mlflow\")\n",
    "\n",
    "# Start a new MLflow run to track the experiment\n",
    "with mlflow.start_run():\n",
    "    # Optimize the hyperparameters using Optuna\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    # Get the best hyperparameters and score\n",
    "    best_params = study.best_params\n",
    "    best_score = study.best_value\n",
    "    \n",
    "# Save the best model in MLflow\n",
    "with mlflow.start_run(nested=True):\n",
    "    # Train the best pipeline on the full dataset\n",
    "    best_pipeline = best_pipelines[best_algo][\"pipeline\"]\n",
    "    best_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    cv_score,roc_auc,average_precision,accuracy,precision,recall,f1 = evaluate_model(best_pipeline, X, y, X_test, y_test)\n",
    "\n",
    "    # Log the pipeline and its parameters\n",
    "    mlflow.sklearn.log_model(best_pipeline, \"best_model\")\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"mean_cv_score\", cv_score)\n",
    "    mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "    mlflow.log_metric(\"average_precision\", average_precision)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1 score\", f1)\n",
    "\n",
    "    # Save the best model as a joblib file\n",
    "    joblib.dump(best_pipeline, \"best_model.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
